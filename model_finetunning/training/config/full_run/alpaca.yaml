model_name: "unsloth/mistral-7b"
peft_type: "lora"
load_in_4bit: true

training_arguments:
  output_dir: "./dry_run_output"
  max_steps: 1                  # Dry run - only one training step
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  evaluation_strategy: "no"
  save_strategy: "no"
  logging_steps: 1
  report_to: "none"
  remove_unused_columns: false
  fp16: true
  gradient_accumulation_steps: 1
  learning_rate: 1e-4
  logging_first_step: true
  run_name: "unsloth_dry_run-run"

peft_config:
  r: 128
  lora_alpha: 32
  lora_dropout: 0
  lora_dropout = 0,
  bias = "none",
  use_gradient_checkpointing = "unsloth",
  random_state = 3407,
  use_rslora = True,
  loftq_config = None,
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - embed_tokens
    - lm_head

push_callback:
  repo_name: "aparadkar/finetuned-model-hindi"
  push_interval_steps: 2
  keep_last_n: 1
  private: true


  model,
    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",

                      "embed_tokens", "lm_head",], # Add for continual pretraining
    lora_alpha = 32,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = True,   # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ